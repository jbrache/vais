{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Author: Jose Brache \\\n",
    " Email: jbrache@google.com \\\n",
    "<img src=\"img/google-cloud-icon.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training application package\n",
    "\n",
    "The easiest (and recommended) way to create a training application package uses gcloud to package and upload the application when you submit your training job. This method allows you to create a very simple file structure. For this tutorial, the file structure of your training application package should appear similar to the following:\n",
    "\n",
    "```\n",
    "config/\n",
    "    config.yaml\n",
    "trainer/ \n",
    "    __init__.py\n",
    "    task.py\n",
    "    model.py\n",
    "    metadata.py\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Your Application Name, Task Name, and Directories.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Name:      custom-py-pkg\n",
      "Task Directory: ./custom-py-pkg\n",
      "Python Package Directory: custom-py-pkg/trainer\n"
     ]
    }
   ],
   "source": [
    "TASK_TYPE = \"custom-py-pkg\"\n",
    "TASK_NAME = f\"{TASK_TYPE}\"\n",
    "TASK_DIR = f\"./{TASK_NAME}\"\n",
    "PYTHON_PACKAGE_APPLICATION_DIR = f\"{TASK_NAME}/trainer\"\n",
    "\n",
    "print(f\"Task Name:      {TASK_NAME}\")\n",
    "print(f\"Task Directory: {TASK_DIR}\")\n",
    "print(f\"Python Package Directory: {PYTHON_PACKAGE_APPLICATION_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf_trainer directory and load the trainer files in it\n",
    "!mkdir -p $PYTHON_PACKAGE_APPLICATION_DIR\n",
    "!touch $PYTHON_PACKAGE_APPLICATION_DIR/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./custom-py-pkg/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TASK_DIR}/setup.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "# REQUIRED_PACKAGES = [\n",
    "#     'tensorflow==2.1.0',\n",
    "#     'numpy==1.18.0',\n",
    "#     'pandas==1.2.1',\n",
    "#     'scipy==1.4.1',\n",
    "#     'scikit-learn==0.22',\n",
    "#     'google-cloud-storage==1.23.0',\n",
    "#     'xgboost==1.3.3',\n",
    "#     'cloudml-hypertune',\n",
    "#     ]\n",
    " \n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=(),    # set install_requires=REQUIRED_PACKAGES, to specify the required packages\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Trainer package for scikit-learn Task'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom-py-pkg/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PYTHON_PACKAGE_APPLICATION_DIR}/__init__.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your training code (Example showed here is to use scikit-learn to classify structured mortgage data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom-py-pkg/trainer/util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PYTHON_PACKAGE_APPLICATION_DIR}/util.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Hold utility functions.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import model_selection as ms\n",
    "#from sklearn.externals import joblib\n",
    "import joblib\n",
    "from trainer import metadata\n",
    "\n",
    "def data_train_test_split(data_df):\n",
    "    \"\"\"Split the DataFrame two subsets for training and testing.\n",
    "    Args:\n",
    "      data_df: (pandas.DataFrame) DataFrame the splitting to be performed on\n",
    "    Returns:\n",
    "      A Tuple of (pandas.DataFrame, pandas.Series,\n",
    "                  pandas.DataFrame, pandas.Series)\n",
    "    \"\"\"\n",
    "\n",
    "    if metadata.FEATURE_NAMES is None:\n",
    "        # Use all the columns as features, except for the target column\n",
    "        feature_names = list(data_df.columns)\n",
    "        feature_names.remove(metadata.TARGET_NAME)\n",
    "        features = data_df[feature_names]\n",
    "    else:\n",
    "        # Only use metadata.FEATURE_NAMES\n",
    "        features = data_df[metadata.FEATURE_NAMES]\n",
    "    target = data_df[metadata.TARGET_NAME]\n",
    "\n",
    "    x_train, x_val, y_train, y_val = ms.train_test_split(features,\n",
    "                                                         target,\n",
    "                                                         test_size=0.2)\n",
    "    return x_train.values, y_train, x_val.values, y_val\n",
    "\n",
    "\n",
    "def read_df_from_bigquery(full_table_path, project_id=None, num_samples=None):\n",
    "    \"\"\"Read data from BigQuery and split into train and validation sets.\n",
    "    Args:\n",
    "      full_table_path: (string) full path of the table containing training data\n",
    "        in the format of [project_id.dataset_name.table_name].\n",
    "      project_id: (string, Optional) Google BigQuery Account project ID.\n",
    "      num_samples: (int, Optional) Number of data samples to read.\n",
    "    Returns:\n",
    "      pandas.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    query = metadata.BASE_QUERY.format(table=full_table_path)\n",
    "    limit = ' LIMIT {}'.format(num_samples) if num_samples else ''\n",
    "    query += limit\n",
    "\n",
    "    # Use \"application default credentials\"\n",
    "    # Use SQL syntax dialect\n",
    "    data_df = pd.read_gbq(query, project_id=project_id, dialect='standard')\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def read_df_from_gcs(file_pattern):\n",
    "    \"\"\"Read data from Google Cloud Storage, split into train and validation sets\n",
    "    Assume that the data on GCS is in csv format without header.\n",
    "    The column names will be provided through metadata\n",
    "    Args:\n",
    "      file_pattern: (string) pattern of the files containing training data.\n",
    "      For example: [gs://bucket/folder_name/prefix]\n",
    "    Returns:\n",
    "      pandas.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Download the files to local /tmp/ folder\n",
    "    df_list = []\n",
    "\n",
    "    for filepath in tf.io.gfile.glob(file_pattern):\n",
    "        with tf.io.gfile.GFile(filepath, 'r') as f:\n",
    "            if metadata.CSV_COLUMNS is None:\n",
    "                df_list.append(pd.read_csv(f))\n",
    "            else:\n",
    "                df_list.append(pd.read_csv(f, names=metadata.CSV_COLUMNS,\n",
    "                                           header=None))\n",
    "\n",
    "    data_df = pd.concat(df_list)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def upload_to_gcs(local_path, gcs_path):\n",
    "    \"\"\"Upload local file to Google Cloud Storage.\n",
    "    Args:\n",
    "      local_path: (string) Local file\n",
    "      gcs_path: (string) Google Cloud Storage destination\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    tf.io.gfile.copy(local_path, gcs_path)\n",
    "\n",
    "\n",
    "def dump_object(object_to_dump, output_path):\n",
    "    \"\"\"Pickle the object and save to the output_path.\n",
    "    Args:\n",
    "      object_to_dump: Python object to be pickled\n",
    "      output_path: (string) output path which can be Google Cloud Storage\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "\n",
    "    if not tf.io.gfile.exists(output_path):\n",
    "        tf.io.gfile.makedirs(os.path.dirname(output_path))\n",
    "    with tf.io.gfile.GFile(output_path, 'w') as wf:\n",
    "        joblib.dump(object_to_dump, wf)\n",
    "\n",
    "\n",
    "def boolean_mask(columns, target_columns):\n",
    "    \"\"\"Create a boolean mask indicating location of target_columns in columns.\n",
    "    Args:\n",
    "      columns: (List[string]), list of all columns considered.\n",
    "      target_columns: (List[string]), columns whose position\n",
    "        should be masked as 1.\n",
    "    Returns:\n",
    "      List[bool]\n",
    "    \"\"\"\n",
    "    target_set = set(target_columns)\n",
    "    return [x in target_set for x in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom-py-pkg/trainer/metadata.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PYTHON_PACKAGE_APPLICATION_DIR}/metadata.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Dataset metadata.\"\"\"\n",
    "\n",
    "# If the input CSV file has a header row, then set CSV_COLUMNS to None.\n",
    "# Otherwise, set CSV_COLUMNS to a list of target and feature names:\n",
    "# i.e. CSV_COLUMNS = None\n",
    "CSV_COLUMNS = [\n",
    "    'dimension_1',\n",
    "    'dimension_2'\n",
    "]\n",
    "\n",
    "# Target name\n",
    "# i.e. TARGET_NAME = 'tip'\n",
    "TARGET_NAME = None\n",
    "\n",
    "# The features to be used for training.\n",
    "# If FEATURE_NAMES is None, then all the available columns will be\n",
    "# used as features, except for the target column.\n",
    "# i.e. FEATURE_NAMES = ['trip_miles','trip_seconds','fare','trip_start_month','trip_start_hour','trip_start_day',]\n",
    "FEATURE_NAMES = None\n",
    "\n",
    "# If the model is serialized using joblib\n",
    "# then use 'model.joblib' for the model name\n",
    "MODEL_FILE_NAME = 'model.joblib'\n",
    "\n",
    "# Set to True if you want to tune some hyperparameters\n",
    "HYPERPARAMTER_TUNING = False\n",
    "\n",
    "# Used only if the dataset is to be read from BigQuery\n",
    "BASE_QUERY = '''\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      `{table}`\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom-py-pkg/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PYTHON_PACKAGE_APPLICATION_DIR}/model.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"ML model definitions.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# this is just a Tensorflow example has a more powerful example\n",
    "# https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/self-paced-labs/ai-platform-qwikstart/ai_platform_qwik_start.ipynb\n",
    "\n",
    "def get_estimator(arguments):\n",
    "    \"\"\"Create an Isolation Forest classifier for anomaly detection \n",
    "    # Generate ML Pipeline which include both pre-processing and model training\n",
    "    \n",
    "    Args:\n",
    "      arguments: (argparse.ArgumentParser), parameters passed from command-line\n",
    "    Returns:\n",
    "      classifier - the Isolation Forests classifier(still needs to be trained)\n",
    "    \"\"\"\n",
    "\n",
    "    # max_samples and random_state_seed are expected to be passed as\n",
    "    # command line argument to task.py\n",
    "    \n",
    "    # max_samples: “auto”, int or float, default=”auto”\n",
    "    # The number of samples to draw from X to train each base estimator.\n",
    "    \n",
    "    # random_stateint, RandomState instance or None, default=None\n",
    "    # Controls the pseudo-randomness of the selection of the feature and split values for each branching step and each tree in the forest.\n",
    "    \n",
    "    estimator = IsolationForest(\n",
    "        max_samples=arguments.max_samples,\n",
    "        random_state=arguments.random_state_seed)\n",
    "\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom-py-pkg/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PYTHON_PACKAGE_APPLICATION_DIR}/task.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Executes model training and evaluation.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "#import hypertune\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from trainer import model\n",
    "from trainer import util\n",
    "from trainer import metadata\n",
    "\n",
    "def train_and_evaluate(estimator, dataset, output_dir):\n",
    "    \"\"\"Runs model training and evaluation.\n",
    "    Args:\n",
    "      estimator: (pipeline.Pipeline), Pipeline instance, assemble pre-processing\n",
    "        steps and model training\n",
    "      dataset: (pandas.DataFrame), DataFrame containing training data\n",
    "      output_dir: (string), directory that the trained model will be exported\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    #x_train, y_train, x_val, y_val = util.data_train_test_split(dataset)\n",
    "    x_train = dataset\n",
    "\n",
    "    estimator.fit(x_train)\n",
    "\n",
    "    # Write model and eval metrics to `output_dir`\n",
    "    model_output_path = os.path.join(output_dir,\n",
    "                                     metadata.MODEL_FILE_NAME)\n",
    "\n",
    "    util.dump_object(estimator, model_output_path)\n",
    "\n",
    "def run_experiment(arguments):\n",
    "    \"\"\"Testbed for running model training and evaluation.\"\"\"\n",
    "    # Get data for training and evaluation\n",
    "\n",
    "    logging.info('Arguments: %s', arguments)\n",
    "    \n",
    "    # Get the training data\n",
    "    dataset_df = util.read_df_from_gcs(arguments.input)\n",
    "    dataset = dataset_df.to_numpy()\n",
    "\n",
    "    # Get estimator\n",
    "    estimator = model.get_estimator(arguments)\n",
    "\n",
    "    # Run training and evaluation\n",
    "    train_and_evaluate(estimator, dataset, arguments.job_dir)\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parses command-line arguments.\"\"\"\n",
    "    \"\"\"Argument parser.\n",
    "\n",
    "    Returns:\n",
    "      Dictionary of arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--log-level',\n",
    "        help='Logging level.',\n",
    "        choices=[\n",
    "            'DEBUG',\n",
    "            'ERROR',\n",
    "            'FATAL',\n",
    "            'INFO',\n",
    "            'WARN',\n",
    "        ],\n",
    "        default='INFO',\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--input',\n",
    "        help='''Dataset to use for training and evaluation.\n",
    "              Can be BigQuery table or a file (CSV).\n",
    "              If BigQuery table, specify as as PROJECT_ID.DATASET.TABLE_NAME.\n",
    "            ''',\n",
    "        required=True,\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='Output directory for exporting model and other metadata.',\n",
    "        required=True,\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--max-samples',\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help='maximum number of random samples to generate, default=100')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--random-state-seed',\n",
    "        type=int,\n",
    "        default=42,\n",
    "        help='random seed used to initialize the pseudo-random number generator, default=42')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--n-estimators',\n",
    "        help='Number of trees in the forest.',\n",
    "        default=10,\n",
    "        type=int,\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--max-depth',\n",
    "        help='The maximum depth of the tree.',\n",
    "        type=int,\n",
    "        default=3,\n",
    "    )\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"Entry point\"\"\"\n",
    "\n",
    "    arguments = parse_args()\n",
    "    logging.basicConfig(level=arguments.log_level)\n",
    "    # Run the train and evaluate experiment\n",
    "    time_start = datetime.utcnow()\n",
    "    run_experiment(arguments)\n",
    "    time_end = datetime.utcnow()\n",
    "    time_elapsed = time_end - time_start\n",
    "    logging.info('Experiment elapsed time: {} seconds'.format(\n",
    "        time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure for Vertex AI Training\n",
    "Create config file for Cloud AI Platform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the config directory and load the trainer files in it\n",
    "!mkdir -p $TASK_NAME/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom-py-pkg/config/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $TASK_NAME/config/config.yaml\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: n1-highmem-8\n",
    "#  masterConfig:\n",
    "#    acceleratorConfig:\n",
    "#      count: 1\n",
    "#      type: NVIDIA_TESLA_T4\n",
    "\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, open the **sklearn-pb-ctr.ipynb** notebook"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
