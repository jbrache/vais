{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671563a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4416e6e8",
   "metadata": {},
   "source": [
    "With our container pushed to Container Registry, we're now ready to kick off a custom model training job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade0935",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5bf230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from googleapiclient import discovery\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b3e3e",
   "metadata": {},
   "source": [
    "### Configure Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bbd512",
   "metadata": {},
   "source": [
    "List your current GCP project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = !gcloud config list --format 'value(core.project)' 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7719ae",
   "metadata": {},
   "source": [
    "Configure your system variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e48f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your global variables\n",
    "PROJECT = project_id[0]          # Replace with your project ID\n",
    "USER = 'test_user'               # Replace with your user name\n",
    "BUCKET_NAME = project_id[0] + '-vertex-ai'       # Replace with your gcs bucket name\n",
    "\n",
    "FOLDER_NAME = 'sklearn_models'\n",
    "ALGORITHM = 'isolation_forest'\n",
    "TIMEZONE = 'US/Pacific'         \n",
    "REGION = 'us-central1'           # bucket should be in same region as Vertex AI         \n",
    "PACKAGE_URIS = f\"gs://{BUCKET_NAME}/trainer/{FOLDER_NAME}/{ALGORITHM}/trainer-0.1.tar.gz\" \n",
    "TRAIN_FEATURE_PATH = f\"gs://{BUCKET_NAME}/{FOLDER_NAME}_data/{ALGORITHM}/train/train.csv\"\n",
    "TEST_FEATURE_PATH = f\"gs://{BUCKET_NAME}/{FOLDER_NAME}_data/{ALGORITHM}/test/test.csv\"\n",
    "\n",
    "IMAGE_URI=f\"gcr.io/{PROJECT}/sklearn_isolation_forest:v1\"\n",
    "print(f\"Container URI: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Project:      {PROJECT}\")\n",
    "print(f\"Bucket Name: {BUCKET_NAME}\")\n",
    "print(f\"Python Package URI: {PACKAGE_URIS}\")\n",
    "print(f\"Training Data URI: {TRAIN_FEATURE_PATH}\")\n",
    "print(f\"Python Package URI: {TEST_FEATURE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0493286",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = f\"./data\"\n",
    "\n",
    "print(f\"Data Directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6340241c",
   "metadata": {},
   "source": [
    "------\n",
    "### Step 1: Kick off the training job\n",
    "Vertex AI gives you two options for training models:\n",
    "\n",
    "* **AutoML**: Train high-quality models with minimal effort and ML expertise.\n",
    "* **Custom training**: Run your custom training applications in the cloud using one of Google Cloud's pre-built containers or use your own.\n",
    "In this lab, we're using custom training via our own custom container on Google Container Registry.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43386f5d",
   "metadata": {},
   "source": [
    "Configure your system variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb1ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Cloud AI Platform requires each job to have unique name, \n",
    "# Therefore, we use prefix + timestamp to form job names.\n",
    "JOB_NAME = 'custom_container_isolation_forest_{}'.format(\n",
    "    datetime.now(timezone(TIMEZONE)).strftime(\"%m%d%y_%H%M\")\n",
    "    )\n",
    "# We use the job names as folder names to store outputs.\n",
    "JOB_DIR = 'gs://{}/{}/{}'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME,\n",
    "    JOB_NAME,\n",
    "    )\n",
    "\n",
    "#JOB_DIR = f\"gs://{BUCKET_NAME}/{FOLDER_NAME}/training_container_isolation_forest\"\n",
    "\n",
    "print(\"JOB_NAME = \", JOB_NAME)\n",
    "print(\"JOB_DIR = \", JOB_DIR)\n",
    "\n",
    "MAX_SAMPLES = '100'  # No of samples\n",
    "RANDOM_STATE_SEED = '42'\n",
    "\n",
    "print(\"MAX_SAMPLES = \", MAX_SAMPLES)\n",
    "print(\"RANDOM_STATE_SEED = \", RANDOM_STATE_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4819d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_CONTAINER_IMAGE_URI = IMAGE_URI\n",
    "api_endpoint = \"us-central1-aiplatform.googleapis.com\"\n",
    "MACHINE_TYPE = \"n1-standard-4\"\n",
    "REPLICA_COUNT = 1\n",
    "\n",
    "# The AI Platform services require regional API endpoints.\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "# Initialize client that will be used to create and send requests.\n",
    "# This client only needs to be created once, and can be reused for multiple requests.\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "custom_job = {\n",
    "    \"display_name\": JOB_NAME,\n",
    "    \"job_spec\": {\n",
    "        \"worker_pool_specs\": [\n",
    "            {\n",
    "                \"machine_spec\": {\n",
    "                    \"machine_type\": MACHINE_TYPE,\n",
    "                },\n",
    "                \"replica_count\": 1,\n",
    "                \"container_spec\": {\n",
    "                    \"image_uri\": CUSTOM_CONTAINER_IMAGE_URI,\n",
    "                    \"command\": [],\n",
    "                    \"args\": [\n",
    "                      '--input',\n",
    "                      TRAIN_FEATURE_PATH,\n",
    "                      '--job-dir',\n",
    "                      JOB_DIR,\n",
    "                      '--max-samples',\n",
    "                      MAX_SAMPLES,\n",
    "                      '--random-state-seed',\n",
    "                      RANDOM_STATE_SEED\n",
    "                    ],\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "parent = f\"projects/{PROJECT}/locations/{REGION}\"\n",
    "response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "print(\"response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a55b55",
   "metadata": {},
   "source": [
    "**The training job will take about 10-15 minutes to complete.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3925a06",
   "metadata": {},
   "source": [
    "Check the training job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d35c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the training job status\n",
    "job_id_trn = response.name.split('/')[-1]\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "name = client.custom_job_path(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    custom_job=job_id_trn,\n",
    ")\n",
    "response = client.get_custom_job(name=name)\n",
    "print(response.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79dc701",
   "metadata": {},
   "source": [
    "More information on using containers for prediction:\n",
    "https://cloud.google.com/vertex-ai/docs/predictions/use-custom-container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde2439",
   "metadata": {},
   "source": [
    "------\n",
    "### 2. Deploy a model endpoint\n",
    "When we set up our training job, we specified where Vertex AI should look for our exported model assets. As part of our training pipeline, Vertex will create a model resource based on this asset path. The model resource itself isn't a deployed model, but once you have a model you're ready to deploy it to an endpoint. To learn more about Models and Endpoints in Vertex AI, check out the [documentation](https://cloud.google.com/vertex-ai/docs/start).\n",
    "\n",
    "In this step we'll create an endpoint for our trained model. We can use this to get predictions on our model via the Vertex AI API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20758bd2",
   "metadata": {},
   "source": [
    "#### Step 1: Import model artifacts to Vertex AI \n",
    "\n",
    "When you import a model, you associate it with a container for Vertex AI to run prediction requests. You can use pre-built containers provided by Vertex AI, or use your own custom containers that you build and push to Container Registry or Artifact Registry.\n",
    "\n",
    "You can use a pre-built container if your model meets the following requirements:\n",
    "\n",
    "- Trained in Python 3.7 or later\n",
    "- Trained using TensorFlow, scikit-learn, or XGBoost\n",
    "- Exported to meet framework-specific requirements for one of the pre-built prediction containers\n",
    "\n",
    "The link to the list of pre-built predict container images:\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589762e8",
   "metadata": {},
   "source": [
    "#### Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db58c9",
   "metadata": {},
   "source": [
    "When we set up our training job, we could also setup a training pipeline and specify where Vertex AI should look for our exported model assets. As part of our training pipeline, Vertex will create a model resource based on this asset path. The model resource itself isn't a deployed model, but once you have a model you're ready to deploy it to an endpoint. To learn more about Models and Endpoints in Vertex AI, check out the [documentation](https://cloud.google.com/vertex-ai/docs/start).\n",
    "\n",
    "In this step we'll create the model, specifying where Vertex AI should look for our exported model assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d9037",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"pre_built_container_isolation_forest\"\n",
    "\n",
    "response = aiplatform.Model.upload(\n",
    "    display_name = MODEL_NAME,\n",
    "    serving_container_image_uri = 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest',\n",
    "    artifact_uri = JOB_DIR\n",
    ")\n",
    "\n",
    "model_id = response.name.split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb6c35",
   "metadata": {},
   "source": [
    "#### Step 2: Create Endpoint\n",
    "\n",
    "You need the endpoint ID to deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b8c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ENDPOINT_DISPLAY_NAME = \"pre_built_container_isolation_forest_endpoint\"\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=MODEL_ENDPOINT_DISPLAY_NAME, project=PROJECT, location=REGION,\n",
    ")\n",
    "\n",
    "endpoint_id = endpoint.resource_name.split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929de18",
   "metadata": {},
   "source": [
    "#### Step 3: Deploy Model to the endpoint\n",
    "\n",
    "You must deploy a model to an endpoint before that model can be used to serve online predictions; deploying a model associates physical resources with the model so it can serve online predictions with low latency. An undeployed model can serve batch predictions, which do not have the same low latency requirements.\n",
    "\n",
    "Deploying the endpoint will take 10-15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe228f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYED_MODEL_DISPLAY_NAME = \"pre_built_container_isolation_forest_deployed\"\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "model = aiplatform.Model(model_name=model_id)\n",
    "\n",
    "# The explanation_metadata and explanation_parameters should only be\n",
    "# provided for a custom trained model and not an AutoML model.\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=DEPLOYED_MODEL_DISPLAY_NAME,\n",
    "    machine_type = \"n1-standard-4\",\n",
    "    sync=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988df37c",
   "metadata": {},
   "source": [
    "------\n",
    "### 3. Send inference requests to your model\n",
    "\n",
    "Vertex AI provides the services you need to request predictions from your model in the cloud.\n",
    "\n",
    "There are two ways to get predictions from trained models: online prediction (sometimes called HTTP prediction) and batch prediction. In both cases, you pass input data to a cloud-hosted machine-learning model and get inferences for each data instance.\n",
    "\n",
    "Vertex AI online prediction is a service optimized to run your data through hosted models with as little latency as possible. You send small batches of data to the service and it returns your predictions in the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4e41d0",
   "metadata": {},
   "source": [
    "#### Load testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f8baf",
   "metadata": {},
   "source": [
    "#### Call Google API for online inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa2fcc",
   "metadata": {},
   "source": [
    "We'll get predictions on our trained model using the Vertex Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4607c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import errors\n",
    "import pandas as pd\n",
    "\n",
    "# Load test feature and labels\n",
    "x_test = pd.read_csv(TEST_FEATURE_PATH)\n",
    "\n",
    "# Fill nan value with zeros (Prediction lacks the ability to handle nan values for now)\n",
    "x_test = x_test.fillna(0)\n",
    "\n",
    "pprobas = []\n",
    "batch_size = 10\n",
    "n_samples = min(160,x_test.shape[0])\n",
    "print(\"batch_size=\", batch_size)\n",
    "print(\"n_samples=\", n_samples)\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "for i in range(0, n_samples, batch_size):\n",
    "    j = min(i+batch_size, n_samples)\n",
    "    print(\"Processing samples\", i, j)\n",
    "    response = aiplatform.Endpoint(endpoint_id).predict(instances=x_test.iloc[i:j].values.tolist())\n",
    "    try:\n",
    "        for prediction_ in response.predictions:\n",
    "            pprobas.append(prediction_)\n",
    "    except errors.HttpError as err:\n",
    "        # Something went wrong, print out some information.\n",
    "        print('There was an error getting the job info, Check the details:')\n",
    "        print(err._get_reason())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fbc8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprobas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6ffa30",
   "metadata": {},
   "source": [
    "🎉 Congratulations! 🎉\n",
    "\n",
    "You've learned how to use Vertex AI to:\n",
    "\n",
    "Train a model by providing the training code in a custom container. You used a scikit-learn model in this example, but you can train a model built with any framework using custom containers.\n",
    "Deploy a scikit-learn model using a pre-built container as part of the same workflow you used for training.\n",
    "Create a model endpoint and generate a prediction.\n",
    "To learn more about different parts of Vertex, check out the documentation.\n",
    "https://cloud.google.com/vertex-ai/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79f44d1",
   "metadata": {},
   "source": [
    "------\n",
    "# Extra examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd45a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "model_instances = aiplatform.Model.list(\n",
    "    filter='display_name=\"pre_built_container_isolation_forest\"'\n",
    ")\n",
    "for resource in model_instances:\n",
    "    #print(dir(resource))\n",
    "    print(resource.display_name)\n",
    "    print(resource.resource_name)\n",
    "    model_name = resource.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62fee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "endpoint_instances = aiplatform.Endpoint.list(\n",
    "    filter='display_name=\"pre_built_container_isolation_forest_endpoint\"'\n",
    ")\n",
    "for resource in endpoint_instances:\n",
    "    #print(dir(resource))\n",
    "    print(resource.display_name)\n",
    "    print(resource.resource_name)\n",
    "    endpoint_name = resource.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a6d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_json(endpoint_name, csv_path, version=None):\n",
    "    \"\"\"Send json data to a deployed model for prediction.\n",
    "\n",
    "    Args:\n",
    "        endpoint_name (str): Required. A fully-qualified endpoint resource name or endpoint ID. \n",
    "            Example: “projects/123/locations/us-central1/endpoints/456”\n",
    "        csv_path (str): path to CSV file to run predictions;\n",
    "        endpoint (str): model name.\n",
    "        instances ([Mapping[str: Any]]): Keys should be the names of Tensors\n",
    "            your deployed model expects as inputs. Values should be datatypes\n",
    "            convertible to Tensors, or (potentially nested) lists of datatypes\n",
    "            convertible to tensors.\n",
    "        versio (str): Optional, version of the model to target.\n",
    "    Returns:\n",
    "        Mapping[str: any]: dictionary of prediction results defined by the\n",
    "            model.\n",
    "    \"\"\"\n",
    "    \n",
    "    from googleapiclient import errors\n",
    "    import pandas as pd\n",
    "\n",
    "    # Load test feature and labels\n",
    "    x_test = pd.read_csv(csv_path)\n",
    "\n",
    "    # Fill nan value with zeros (Prediction lacks the ability to handle nan values for now)\n",
    "    x_test = x_test.fillna(0)\n",
    "\n",
    "    pprobas = []\n",
    "    batch_size = 10\n",
    "    n_samples = min(160,x_test.shape[0])\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"n_samples=\", n_samples)\n",
    "\n",
    "    #aiplatform.init(project=project, location=region)\n",
    "\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        j = min(i+batch_size, n_samples)\n",
    "        print(\"Processing samples\", i, j)\n",
    "        response = aiplatform.Endpoint(endpoint_name).predict(instances=x_test.iloc[i:j].values.tolist())\n",
    "        try:\n",
    "            for prediction_ in response.predictions:\n",
    "                pprobas.append(prediction_)\n",
    "        except errors.HttpError as err:\n",
    "            # Something went wrong, print out some information.\n",
    "            tf.compat.v1.logging.error('There was an error getting the job info, Check the details:')\n",
    "            tf.compat.v1.logging.error(err._get_reason())\n",
    "            break\n",
    "    return pprobas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399297c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_json(endpoint_name, TEST_FEATURE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a3243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# If the model is serialized using pickle\n",
    "# then use 'model.pkl' for the model name\n",
    "MODEL_FILE_NAME = 'model.joblib'\n",
    "\n",
    "try:\n",
    "    MODEL_DIR = JOB_DIR\n",
    "except NameError:\n",
    "    MODEL_DIR = \"gs://path/to/model/directory\"\n",
    "\n",
    "model_output_path = os.path.join(MODEL_DIR,\n",
    "                                     MODEL_FILE_NAME)\n",
    "\n",
    "print(model_output_path)\n",
    "\n",
    "# Load test feature and labels\n",
    "x_train_pd = pd.read_csv(TRAIN_FEATURE_PATH)\n",
    "x_test_pd = pd.read_csv(TEST_FEATURE_PATH)\n",
    "x_train = x_train_pd.to_numpy()\n",
    "x_test = x_test_pd.to_numpy()\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Generate some abnormal novel observations\n",
    "x_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n",
    "\n",
    "with tf.io.gfile.GFile(model_output_path, 'rb') as rf:\n",
    "    clf = joblib.load(rf)\n",
    "#clf = joblib.load(tf.io.gfile.GFile(model_output_path, 'rb')) alternative method\n",
    "\n",
    "y_pred_train = clf.predict(x_train)\n",
    "y_pred_test = clf.predict(x_test)\n",
    "y_pred_outliers = clf.predict(x_outliers)\n",
    "\n",
    "# plot the line, the samples, and the nearest vectors to the plane\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"IsolationForest\")\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n",
    "\n",
    "b1 = plt.scatter(x_train[:, 0], x_train[:, 1], c='white',\n",
    "                 s=20, edgecolor='k')\n",
    "b2 = plt.scatter(x_test[:, 0], x_test[:, 1], c='green',\n",
    "                 s=20, edgecolor='k')\n",
    "c = plt.scatter(x_outliers[:, 0], x_outliers[:, 1], c='red',\n",
    "                s=20, edgecolor='k')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([b1, b2, c],\n",
    "           [\"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"upper left\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
