{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbeecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4827b4da",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04c0385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82e71de",
   "metadata": {},
   "source": [
    "Adjusted this notebook from this codelab:\n",
    "https://codelabs.developers.google.com/vertex_custom_training_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6091845",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "In this lab, you will use GKE instead of [Vertex AI](https://cloud.google.com/vertex-ai/docs) to train and serve a TensorFlow model using code in a custom container.\n",
    "\n",
    "While we're using scikit-learn for the model code here, you could easily replace it with another framework.\n",
    "\n",
    "What you learn\n",
    "You'll learn how to:\n",
    "\n",
    "Build and containerize model training code in Vertex Notebooks\n",
    "Submit a custom model training job to GKE\n",
    "Deploy your trained model GKE as a job, and use that job to get predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7b906e",
   "metadata": {},
   "source": [
    "## 2. Intro to Vertex AI\n",
    "This lab uses GKE to run training and predictions.\n",
    "\n",
    "Although running training/prediction is an option, consider the newest AI product offering available on Google Cloud. [Vertex AI](https://cloud.google.com/vertex-ai/docs) integrates the ML offerings across Google Cloud into a seamless development experience. Previously, models trained with AutoML and custom models were accessible via separate services. The new offering combines both into a single API, along with other new products. You can also migrate existing projects to Vertex AI. If you have any feedback, please see the [support page](https://cloud.google.com/vertex-ai/docs/support/getting-support).\n",
    "\n",
    "Vertex AI includes many different products to support end-to-end ML workflows. This lab will focus on the products highlighted below: Training, Prediction, and Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd599a76",
   "metadata": {},
   "source": [
    "## 3. Setup your environment\n",
    "You'll need a Google Cloud Platform project with billing enabled to run this codelab. To create a project, follow the [instructions here](https://cloud.google.com/resource-manager/docs/creating-managing-projects)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3878d7c3",
   "metadata": {},
   "source": [
    "### Step 3.1: Enable the Compute Engine API\n",
    "Navigate to [Compute Engine](https://console.cloud.google.com/marketplace/details/google/compute.googleapis.com) and select **Enable** if it isn't already enabled. You'll need this to create your notebook instance.\n",
    "### Step 3.2: Enable the Vertex AI API\n",
    "Navigate to the [Vertex AI section of your Cloud Console](https://console.cloud.google.com/ai/platform) and click **Enable** Vertex AI API.\n",
    "### Step 3.3: Enable the Container Registry API\n",
    "Navigate to the [Container Registry](https://console.cloud.google.com/apis/library/containerregistry.googleapis.com) and select **Enable** if it isn't already. You'll use this to create a container for your custom training job.\n",
    "### Step 3.4: Create an Vertex Notebooks instance\n",
    "From the [Vertex AI section](https://console.cloud.google.com/ai/platform) of your Cloud Console, click on Notebooks.\n",
    "\n",
    "From there, select **New Instance**. Then select the **TensorFlow Enterprise 2.3** instance type **without GPUs**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fbd665",
   "metadata": {},
   "source": [
    "## 4. Containerize training code\n",
    "We'll submit this training job to Vertex by putting our training code in a [Docker container](https://www.docker.com/resources/what-container) and pushing this container to [Google Container Registry](https://cloud.google.com/container-registry). Using this approach, we can train a model built with any framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71994fb3",
   "metadata": {},
   "source": [
    "Create a new directory to packaget the container, in this case *sklearn_container* and cd into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "200dcb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Name:      sklearn_container_training\n",
      "Task Directory: ./sklearn_container_training\n",
      "Python Package Directory: sklearn_container_training/trainer\n"
     ]
    }
   ],
   "source": [
    "CONTAINER_DIR = \"sklearn_container_training\"\n",
    "TASK_TYPE = \"sklearn_container_training\"\n",
    "TASK_NAME = f\"{TASK_TYPE}\"\n",
    "TASK_DIR = f\"./{TASK_NAME}\"\n",
    "PYTHON_PACKAGE_APPLICATION_DIR = f\"{TASK_NAME}/trainer\"\n",
    "\n",
    "print(f\"Task Name:      {TASK_NAME}\")\n",
    "print(f\"Task Directory: {TASK_DIR}\")\n",
    "print(f\"Python Package Directory: {PYTHON_PACKAGE_APPLICATION_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2483c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the container directory\n",
    "!mkdir -p $CONTAINER_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e06e2e0",
   "metadata": {},
   "source": [
    "### Step 4.1: Create a Dockerfile\n",
    "Our first step in containerizing our code is to create a Dockerfile. In our Dockerfile we'll include all the commands needed to run our image. It'll install all the libraries we're using and set up the entry point for our training code. From your Terminal, create an empty Dockerfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "402cb792",
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch $CONTAINER_DIR/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c456d9",
   "metadata": {},
   "source": [
    "Open the Dockerfile and copy the following into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b431ae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sklearn_container_training/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile $CONTAINER_DIR/Dockerfile\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3\n",
    "WORKDIR /root\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# Install pip reqs from both user and default\n",
    "# NOTE: for this implementation, requirements.txt specifies \n",
    "#   the tornado, scikit-learn, and joblib libraries in \n",
    "#   the format: [library]==[version]. Build the requirements.txt\n",
    "#   file to match your needs\n",
    "#RUN pip install -r requirements.txt\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a59098",
   "metadata": {},
   "source": [
    "This Dockerfile uses the [Deep Learning Container TensorFlow Enterprise 2.3 Docker image](https://cloud.google.com/ai-platform/deep-learning-containers/docs/choosing-container#choose_a_container_image_type). The Deep Learning Containers on Google Cloud come with many common ML and data science frameworks pre-installed. The one we're using includes TF Enterprise 2.3, Pandas, Scikit-learn, and others. After downloading that image, this Dockerfile sets up the entrypoint for our training code. We haven't created these files yet – in the next step, we'll add the code for training and exporting our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab55fd",
   "metadata": {},
   "source": [
    "Additional documentation on creating a custom container image is [here](https://cloud.google.com/vertex-ai/docs/training/create-custom-container)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a19ba",
   "metadata": {},
   "source": [
    "### Step 4.2: Create a Cloud Storage bucket\n",
    "In our training job, we'll export our trained TensorFlow model to a Cloud Storage Bucket. GKE or Vertex AI will use this to read our exported model assets and deploy the model. From your Terminal, run the following to define an env variable for your project, making sure to replace your-cloud-project with the ID of your project:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08b689",
   "metadata": {},
   "source": [
    "You can get your project ID by running gcloud config list --format 'value(core.project)' in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e66f6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = !gcloud config list --format 'value(core.project)' 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4340eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your global variables\n",
    "PROJECT = project_id[0]          # Replace with your project ID\n",
    "BUCKET_NAME = project_id[0] + '-vertex-ai'       # Replace with your gcs bucket name\n",
    "REGION = 'us-central1'\n",
    "\n",
    "FOLDER_NAME = 'sklearn_models'\n",
    "ALGORITHM = 'isolation_forest'\n",
    "TIMEZONE = 'US/Pacific'         \n",
    "REGION = 'us-central1'           # bucket should be in same region as Vertex AI   \n",
    "\n",
    "TRAIN_FEATURE_PATH = f\"gs://{BUCKET_NAME}/{FOLDER_NAME}_data/{ALGORITHM}/train/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "466797e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME =  custom_container_isolation_forest_gke\n",
      "JOB_DIR =  gs://virtual-anomaly-vertex-ai/sklearn_models/custom_container_isolation_forest_gke\n",
      "MAX_SAMPLES =  100\n",
      "RANDOM_STATE_SEED =  42\n"
     ]
    }
   ],
   "source": [
    "# Google Cloud AI Platform requires each job to have unique name, \n",
    "# Therefore, we use prefix + timestamp to form job names.\n",
    "# JOB_NAME = 'custom_container_isolation_forest_{}'.format(\n",
    "#     datetime.now(timezone(TIMEZONE)).strftime(\"%m%d%y_%H%M\")\n",
    "#     )\n",
    "\n",
    "JOB_NAME = 'custom_container_isolation_forest_gke'\n",
    "# We use the job names as folder names to store outputs.\n",
    "JOB_DIR = 'gs://{}/{}/{}'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME,\n",
    "    JOB_NAME,\n",
    "    )\n",
    "\n",
    "print(\"JOB_NAME = \", JOB_NAME)\n",
    "print(\"JOB_DIR = \", JOB_DIR)\n",
    "\n",
    "MAX_SAMPLES = '100'  # No of samples\n",
    "RANDOM_STATE_SEED = '42'\n",
    "\n",
    "print(\"MAX_SAMPLES = \", MAX_SAMPLES)\n",
    "print(\"RANDOM_STATE_SEED = \", RANDOM_STATE_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d5c78",
   "metadata": {},
   "source": [
    "Next, run the following in your Terminal to create a new bucket in your project. The -l (location) flag is important since this needs to be in the same region where you deploy a model endpoint later in the tutorial:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f894d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l $REGION gs://$BUCKET_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18633e01",
   "metadata": {},
   "source": [
    "### Step 4.3: Add model training code\n",
    "From your Terminal, run the following to create a directory for our training code and a Python file where we'll add the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f75998a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $CONTAINER_DIR/trainer\n",
    "!touch $CONTAINER_DIR/trainer/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07753c65",
   "metadata": {},
   "source": [
    "You should now have the following in your sklearn_container/ directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa71913d",
   "metadata": {},
   "source": [
    "+ Dockerfile\n",
    "+ trainer/\n",
    "    + train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca19b40e",
   "metadata": {},
   "source": [
    "Next, open the train.py file you just created and copy the code below (this is adapted from the tutorial in the TensorFlow docs).\n",
    "\n",
    "At the beginning of the file, update the BUCKET variable with the name of the Storage Bucket you created in the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ef8dc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sklearn_container_training/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CONTAINER_DIR}/trainer/train.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import joblib\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# If the input CSV file has a header row, then set CSV_COLUMNS to None.\n",
    "# Otherwise, set CSV_COLUMNS to a list of target and feature names:\n",
    "# i.e. CSV_COLUMNS = None\n",
    "CSV_COLUMNS = [\n",
    "    'dimension_1',\n",
    "    'dimension_2'\n",
    "]\n",
    "\n",
    "# Target name\n",
    "# i.e. TARGET_NAME = 'tip'\n",
    "TARGET_NAME = None\n",
    "\n",
    "# The features to be used for training.\n",
    "# If FEATURE_NAMES is None, then all the available columns will be\n",
    "# used as features, except for the target column.\n",
    "# i.e. FEATURE_NAMES = ['trip_miles','trip_seconds','fare','trip_start_month','trip_start_hour','trip_start_day',]\n",
    "FEATURE_NAMES = None\n",
    "\n",
    "# If the model is serialized using joblib\n",
    "# then use 'model.joblib' for the model name\n",
    "MODEL_FILE_NAME = 'model.joblib'\n",
    "\n",
    "# Set to True if you want to tune some hyperparameters\n",
    "HYPERPARAMTER_TUNING = False\n",
    "\n",
    "def read_df_from_gcs(file_pattern):\n",
    "    \"\"\"Read data from Google Cloud Storage, split into train and validation sets\n",
    "    Assume that the data on GCS is in csv format without header.\n",
    "    The column names will be provided through metadata\n",
    "    Args:\n",
    "      file_pattern: (string) pattern of the files containing training data.\n",
    "      For example: [gs://bucket/folder_name/prefix]\n",
    "    Returns:\n",
    "      pandas.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Download the files to local /tmp/ folder\n",
    "    df_list = []\n",
    "\n",
    "    for filepath in tf.io.gfile.glob(file_pattern):\n",
    "        with tf.io.gfile.GFile(filepath, 'r') as f:\n",
    "            if CSV_COLUMNS is None:\n",
    "                df_list.append(pd.read_csv(f))\n",
    "            else:\n",
    "                df_list.append(pd.read_csv(f, names=CSV_COLUMNS,\n",
    "                                           header=None))\n",
    "\n",
    "    data_df = pd.concat(df_list)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "def upload_to_gcs(local_path, gcs_path):\n",
    "    \"\"\"Upload local file to Google Cloud Storage.\n",
    "    Args:\n",
    "      local_path: (string) Local file\n",
    "      gcs_path: (string) Google Cloud Storage destination\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    tf.io.gfile.copy(local_path, gcs_path)\n",
    "    \n",
    "def dump_object(object_to_dump, output_path):\n",
    "    \"\"\"Pickle the object and save to the output_path.\n",
    "    Args:\n",
    "      object_to_dump: Python object to be pickled\n",
    "      output_path: (string) output path which can be Google Cloud Storage\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "\n",
    "    if not tf.io.gfile.exists(output_path):\n",
    "        tf.io.gfile.makedirs(os.path.dirname(output_path))\n",
    "    with tf.io.gfile.GFile(output_path, 'w') as wf:\n",
    "        joblib.dump(object_to_dump, wf)\n",
    "        \n",
    "def get_estimator(arguments):\n",
    "    \"\"\"Create an Isolation Forest classifier for anomaly detection \n",
    "    # Generate ML Pipeline which include both pre-processing and model training\n",
    "    \n",
    "    Args:\n",
    "      arguments: (argparse.ArgumentParser), parameters passed from command-line\n",
    "    Returns:\n",
    "      classifier - the Isolation Forests classifier(still needs to be trained)\n",
    "    \"\"\"\n",
    "\n",
    "    # max_samples and random_state_seed are expected to be passed as\n",
    "    # command line argument to task.py\n",
    "    \n",
    "    # max_samples: “auto”, int or float, default=”auto”\n",
    "    # The number of samples to draw from X to train each base estimator.\n",
    "    \n",
    "    # random_stateint, RandomState instance or None, default=None\n",
    "    # Controls the pseudo-randomness of the selection of the feature and split values for each branching step and each tree in the forest.\n",
    "    \n",
    "    estimator = IsolationForest(\n",
    "        max_samples=arguments.max_samples,\n",
    "        random_state=arguments.random_state_seed)\n",
    "\n",
    "    return estimator\n",
    "\n",
    "def train_and_evaluate(estimator, dataset, output_dir):\n",
    "    \"\"\"Runs model training and evaluation.\n",
    "    Args:\n",
    "      estimator: (pipeline.Pipeline), Pipeline instance, assemble pre-processing\n",
    "        steps and model training\n",
    "      dataset: (pandas.DataFrame), DataFrame containing training data\n",
    "      output_dir: (string), directory that the trained model will be exported\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    #x_train, y_train, x_val, y_val = util.data_train_test_split(dataset)\n",
    "    x_train = dataset\n",
    "\n",
    "    estimator.fit(x_train)\n",
    "\n",
    "    # Write model and eval metrics to `output_dir`\n",
    "    model_output_path = os.path.join(output_dir,\n",
    "                                     MODEL_FILE_NAME)\n",
    "\n",
    "    dump_object(estimator, model_output_path)\n",
    "    \n",
    "def run_experiment(arguments):\n",
    "    \"\"\"Testbed for running model training and evaluation.\"\"\"\n",
    "    # Get data for training and evaluation\n",
    "\n",
    "    logging.info('Arguments: %s', arguments)\n",
    "    \n",
    "    # Get the training data\n",
    "    logging.info('Getting the training data from: ' + arguments.input)\n",
    "    dataset_df = read_df_from_gcs(arguments.input)\n",
    "    dataset = dataset_df.to_numpy()\n",
    "\n",
    "    # Get estimator\n",
    "    estimator = get_estimator(arguments)\n",
    "\n",
    "    # Run training and evaluation\n",
    "    logging.info('Running training, outputting model to: ' + arguments.job_dir)\n",
    "    train_and_evaluate(estimator, dataset, arguments.job_dir)\n",
    "    \n",
    "def parse_args():\n",
    "    \"\"\"Parses command-line arguments.\"\"\"\n",
    "    \"\"\"Argument parser.\n",
    "\n",
    "    Returns:\n",
    "      Dictionary of arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--log-level', help='Logging level.', choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'], default='INFO')\n",
    "    parser.add_argument('--input', help='CSV file to use for training and evaluation.', required=True)\n",
    "    parser.add_argument('--job-dir', help='Output directory for exporting model and other metadata.', required=True)\n",
    "    parser.add_argument('--max-samples', type=int, default=100, help='maximum number of random samples to generate, default=100')\n",
    "    parser.add_argument('--random-state-seed', type=int, default=42, help='random seed used to initialize the pseudo-random number generator, default=42')\n",
    "    parser.add_argument('--n-estimators', default=10, type=int, help='Number of trees in the forest.')\n",
    "    parser.add_argument('--max-depth', type=int, default=3, help='The maximum depth of the tree.')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"Entry point\"\"\"\n",
    "\n",
    "    arguments = parse_args()\n",
    "    logging.basicConfig(level=arguments.log_level)\n",
    "    #arguments.input = 'gs://cloud-thunderdome-vertex-ai/sklearn_models_data/isolation_forest/train/train.csv'\n",
    "    #arguments.job_dir = 'gs://cloud-thunderdome-vertex-ai/sklearn_models/sklearn_isolation_forest_container'\n",
    "    # Run the train and evaluate experiment\n",
    "    time_start = datetime.utcnow()\n",
    "    run_experiment(arguments)\n",
    "    time_end = datetime.utcnow()\n",
    "    time_elapsed = time_end - time_start\n",
    "    logging.info('Experiment elapsed time: {} seconds'.format(\n",
    "        time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e11d48f",
   "metadata": {},
   "source": [
    "### Step 4.4: Build and test the container locally\n",
    "Define a variable with the URI of your container image in Google Container Registry:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_URI=f\"gcr.io/{PROJECT}/sklearn_isolation_forest_training:v1\"\n",
    "print(f\"Container URI: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef85338",
   "metadata": {},
   "source": [
    "Then, build the container by running the following from the root of your CONTAINER_DIR directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f6daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $CONTAINER_DIR && docker build ./ -t $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737b001",
   "metadata": {},
   "source": [
    "Run the container within your notebook instance to ensure it's working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae59aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $CONTAINER_DIR && docker run $IMAGE_URI --input=$TRAIN_FEATURE_PATH --job-dir=$JOB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538f18f3",
   "metadata": {},
   "source": [
    "The model should finish training in 1-2 minutes. When you've finished running the container locally, push it to Google Container Registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e12c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $CONTAINER_DIR && docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f63d00",
   "metadata": {},
   "source": [
    "With our container pushed to Container Registry, we're now ready to kick off a custom model training job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9343d1-9f40-4ba4-abf8-c56564aab672",
   "metadata": {},
   "source": [
    "### Step 5: Create YAML to run a job on GKE\n",
    "Following below uses cell magic to write out variables stored above for the locations of the file used for inference and where to store model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a546cff5-31a4-4118-99e3-ad8437bb708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "915b778c-2934-4d15-b5cf-4d293f935316",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate k8s_job_training.yaml\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: trainer-job\n",
    "spec:\n",
    "  activeDeadlineSeconds: 1800\n",
    "  template:\n",
    "    metadata:\n",
    "      name: train-anomaly\n",
    "    spec:\n",
    "      serviceAccountName: sa-trainer\n",
    "      containers:\n",
    "      - name: anomaly\n",
    "        image: gcr.io/virtual-anomaly/sklearn_isolation_forest_training:v1\n",
    "        env:\n",
    "          - name: TRAIN_FEATURE_PATH\n",
    "            # Update below to change the input location\n",
    "            value: \"{TRAIN_FEATURE_PATH}\"\n",
    "          - name: JOB_DIR\n",
    "            # Update below to change the output location\n",
    "            value: \"{JOB_DIR}\"\n",
    "        args: [\"--input=$(TRAIN_FEATURE_PATH)\", \"--job-dir=$(JOB_DIR)\"]\n",
    "      restartPolicy: Never\n",
    "  backoffLimit: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e04fe1",
   "metadata": {},
   "source": [
    "### Next, open the **2. sklearn-cb-ctr-setup-prediction** notebook"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
